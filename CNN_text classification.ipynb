{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "#import utils\n",
    "#sudo pip install utils 설치\n",
    "import json\n",
    "\n",
    "\n",
    "BASEURL     = 'http://movie.naver.com/movie/point/af/list.nhn'\n",
    "RATINGURL   = BASEURL + '?&page=%s'\n",
    "MOVIEURL    = BASEURL + '?st=mcode&target=after&sword=%s&page=%s'\n",
    "\n",
    "DATADIR     = 'data/ratings'\n",
    "INDEXFILE   = 'index.txt'\n",
    "TMPFILE     = 'data/ratings_all.txt'\n",
    "RATINGSFILE = 'data/ratings.txt'\n",
    "SEED        = 1234\n",
    "SLEEP       = 600\n",
    "NDOCS       = 200000\n",
    "\n",
    "\n",
    "extract_nums = lambda s: re.search('\\d+', s).group(0)\n",
    "sanitize_str = lambda s: s.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_item(item):\n",
    "    try:\n",
    "        return {'review_id': item.xpath('./td[@class=\"ac num\"]/text()')[0],     # num\n",
    "                'rating': item.xpath('./td[@class=\"point\"]/text()')[0],         # point\n",
    "                'movie_id': extract_nums(item.xpath('./td[@class=\"title\"]/a/@href')[0]),\n",
    "                'review': sanitize_str(' '.join(item.xpath('./td[@class=\"title\"]/text()'))),   \n",
    "                'author': item.xpath('./td[@class=\"num\"]/a/text()')[0],\n",
    "                'date': item.xpath('./td[@class=\"num\"]/text()')[0]\n",
    "        }\n",
    "    except (IndexError, AttributeError) as e:\n",
    "        print(e, item.xpath('.//text()'))\n",
    "        return None\n",
    "    except (AssertionError) as e:\n",
    "        print(e, 'Sleep for %s' % SLEEP)\n",
    "        time.sleep(SLEEP)\n",
    "    except Exception as e:\n",
    "        print(e, '음 여기까진 생각을 못했는데...')\n",
    "\n",
    "\n",
    "def crawl_rating_page(url):\n",
    "    resp = requests.get(url)\n",
    "    root = html.fromstring(resp.text)\n",
    "    items = root.xpath('//body//table[@class=\"list_netizen\"]//tr')[1:]\n",
    "    npages = max(map(int, ([0] + root.xpath('//div[@class=\"paging\"]//a/span/text()'))))\n",
    "    return list(filter(None, [parse_item(item) for item in items])), npages\n",
    "\n",
    "\n",
    "def write_json(items,filenames):\n",
    "    with open(filenames,'w') as outfile:      #wb -> w 로 바꿈.\n",
    "        json.dump(items,outfile)\n",
    "\n",
    "def write_txt(contents, filename):\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(contents)\n",
    "\n",
    "def read_txt(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        x = f.read()\n",
    "        return x\n",
    "\n",
    "def read_json(filenames):\n",
    "    with open(filenames) as f:\n",
    "        return json.loads(f.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crawl_movie(movie_id):\n",
    "    items = []\n",
    "    for page_num in range(10):  # limit to 100 recent ratings per movie\n",
    "        url = MOVIEURL % (movie_id, page_num + 1)\n",
    "        page_items, npages = crawl_rating_page(url)\n",
    "        items.extend(page_items)\n",
    "        if len(items)==0:\n",
    "            return []\n",
    "        if page_num >= npages - 1:\n",
    "            break\n",
    "    if items:\n",
    "        #utils.write_json(items, '%s/%s.json' % (DATADIR, movie_id))\n",
    "        write_json(items, '%s/%s.json' % (DATADIR, movie_id))\n",
    "        return items\n",
    "    else:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_index(filename):\n",
    "    if os.path.exists(filename):\n",
    "        #movie_id, total = map(int, utils.read_txt(filename).split('\\n')[0].split(','))\n",
    "        movie_id, total = map(int, read_txt(filename).split('\\n')[0].split(','))\n",
    "    else:\n",
    "        movie_id, total = 129406, 0\n",
    "    print(movie_id, total)\n",
    "    return [movie_id, total]\n",
    "\n",
    "\n",
    "def put_index(movie_id, total, filename):\n",
    "    #utils.write_txt('%s,%s' % (movie_id, total), filename)\n",
    "    write_txt('%s,%s' % (movie_id, total), filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ratings():\n",
    "\n",
    "    def balance_classes(df, ndocs_per_class):\n",
    "        df_pos = df[df['label']==1][:int(ndocs_per_class)]\n",
    "        df_neg = df[df['label']==0][:int(ndocs_per_class)]\n",
    "        return df_pos.append(df_neg)\n",
    "\n",
    "\n",
    "    sub_space = lambda s: re.sub('\\s+', ' ', s)\n",
    "    write_row = lambda l, f: f.write('\\t'.join(l) + '\\n')\n",
    "\n",
    "    filenames = glob('%s/*' % DATADIR)\n",
    "    with open(TMPFILE, 'w') as f:\n",
    "        write_row('id document label'.split(), f)\n",
    "        for filename in filenames:\n",
    "            #for review in utils.read_json(filename):\n",
    "            for review in read_json(filename):\n",
    "                rating = int(review['rating'])\n",
    "                if rating > 7:      # positive 8, 9, 10       \n",
    "                    write_row([review['review_id'], sub_space(review['review']), '1'], f)\n",
    "                elif rating < 5:    # negative 1, 2, 3, 4\n",
    "                    write_row([review['review_id'], sub_space(review['review']), '0'], f)\n",
    "                else:               # neutral (중립)\n",
    "                    pass\n",
    "    print('Ratings merged to %s' % TMPFILE)\n",
    "\n",
    "    df = pd.read_csv(TMPFILE, sep='\\t', quoting=3)\n",
    "    df = df.fillna('')\n",
    "    np.random.seed(SEED)\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "    df = balance_classes(df, NDOCS/2)\n",
    "    df.to_csv(RATINGSFILE, sep='\\t', index=False)\n",
    "    print('Ratings written to %s' % RATINGSFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104786 90983\n",
      "Ratings merged to data/ratings_all.txt\n",
      "Ratings written to data/ratings.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    movie_id, total = get_index(INDEXFILE)\n",
    "    Ntotal=10000\n",
    "    while total < Ntotal and movie_id > 0:      # 10000대신 원래 작성자는 1000000개로 함.\n",
    "        items = crawl_movie(movie_id)\n",
    "        total += len(items)\n",
    "        put_index(movie_id, total, INDEXFILE)\n",
    "        print(MOVIEURL % (movie_id, 1), len(items), total)\n",
    "        movie_id -= 1\n",
    "    merge_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### test data, train data ######\n",
    "import numpy as np; np.random.seed(1234)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ntrain = 7000      #전체 중 대략 20% 되는 데이터를 test data로.\n",
    "\n",
    "data = pd.read_csv('data/ratings.txt', sep='\\t', quoting=3)\n",
    "data = pd.DataFrame(np.random.permutation(data))\n",
    "trn, tst = data[:ntrain], data[ntrain:]\n",
    "\n",
    "header = 'id document label'.split()\n",
    "trn.to_csv('data/ratings_train.txt', sep='\\t', index=False, header=header)\n",
    "tst.to_csv('data/ratings_test.txt', sep='\\t', index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', '')\n"
     ]
    }
   ],
   "source": [
    "import platform \n",
    "\n",
    "print(platform.architecture())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import konlpy      #sudo pip install konlpy\n",
    "import jpype\n",
    "from konlpy.tag import Twitter\n",
    "#from konlpy.tag import Okt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pos_tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "def read_raw_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        print('loading data')\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "\n",
    "        print('pos tagging to token')\n",
    "        data = [(tokenize(row[1]), int(row[2])) for row in data[1:]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(tokens):\n",
    "    print('building vocabulary')\n",
    "    vocab = dict()\n",
    "    vocab['#UNKOWN'] = 0\n",
    "    vocab['#PAD'] = 1\n",
    "    for t in tokens:\n",
    "        if t not in vocab:\n",
    "            vocab[t] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def get_token_id(token, vocab):\n",
    "    if token in vocab:\n",
    "        return vocab[token]\n",
    "    else:\n",
    "        0 # unkown\n",
    "\n",
    "def build_input(data, vocab):\n",
    "\n",
    "    def get_onehot(index, size):\n",
    "        onehot = [0] * size\n",
    "        onehot[index] = 1\n",
    "        return onehot\n",
    "\n",
    "    print('building input')\n",
    "    result = []\n",
    "    for d in data:\n",
    "        sequence = [get_token_id(t, vocab) for t in d[0]]\n",
    "        while len(sequence) > 0:\n",
    "            seq_seg = sequence[:60]\n",
    "            sequence = sequence[60:]\n",
    "\n",
    "            padding = [1] *(60 - len(seq_seg))\n",
    "            seq_seg = seq_seg + padding\n",
    "\n",
    "            result.append((seq_seg, get_onehot(d[1], 2)))\n",
    "\n",
    "    return result \n",
    "\n",
    "def save_data(filename, data):\n",
    "    def make_csv_str(d):\n",
    "        output = '%d' % d[0]\n",
    "        for index in d[1:]:\n",
    "            output = '%s,%d' % (output, index)\n",
    "        return output\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for d in data:\n",
    "            data_str = make_csv_str(d[0])\n",
    "            label_str = make_csv_str(d[1])\n",
    "            f.write (data_str + '\\n')\n",
    "            f.write (label_str + '\\n')\n",
    "\n",
    "def save_vocab(filename, vocab):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for v in vocab:\n",
    "            f.write('%s\\t%d\\n' % (v, vocab[v]))\n",
    "            \n",
    "def load_data(filename):\n",
    "    result = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(int(len(lines)/2)):\n",
    "            data = lines[i*2]\n",
    "            label = lines[i*2 + 1]\n",
    "\n",
    "            result.append(([int(s) for s in data.split(',')], [int(s) for s in label.split(',')]))\n",
    "    return result\n",
    "\n",
    "def load_vocab(filename):\n",
    "    result = dict()\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            ls = line.split('\\t')\n",
    "            result[ls[0]] = int(ls[1])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = read_raw_data('data/ratings_train.txt')\n",
    "    tokens = [t for d in data for t in d[0]]\n",
    "    vocab = build_vocab(tokens)\n",
    "    d = build_input(data, vocab)\n",
    "    \n",
    "    save_data('data/test_data.txt', d)\n",
    "    save_vocab('data/test_vocab.txt', vocab)\n",
    "\n",
    "    d2 = load_data('data/test_data.txt')\n",
    "    vocab2 = load_vocab('data/test_vocab.txt')\n",
    "\n",
    "    assert(len(d2) == len(d))\n",
    "    for i in range(len(d)):\n",
    "        assert(len(d2[i]) ==  len(d[i]))\n",
    "        for j in range(len(d[i])):\n",
    "            assert(d2[i][j] == d[i][j])\n",
    "\n",
    "    for index in vocab:\n",
    "        assert(vocab2[index] == vocab[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "        # input,  dropout\n",
    "        input = tf.placeholder(tf.int32, [None, sequence_length], name='input')\n",
    "        label = tf.placeholder(tf.float32, [None, num_classes], name='label')\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        with tf.name_scope('embedding'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name='W')\n",
    "            # [None, sequence_length, embedding_size]\n",
    "            embedded_chars = tf.nn.embedding_lookup(W, input)\n",
    "            # [None, sequence_length, embedding_size, 1]\n",
    "            embedded_chars = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                # convolution\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embedded_chars,\n",
    "                    W,\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding='VALID',\n",
    "                    name='conv')\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name='pool')\n",
    "                pooled_outputs.append(pooled)\n",
    "        # \n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs,3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope('dropout'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "        # prediction\n",
    "        with tf.name_scope('output'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            W1 = tf.Variable(tf.random_uniform([num_filters_total, num_classes], -1.0, 1.0))\n",
    "            #W = tf.get_variable('W', shape=[num_filters_total, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n",
    "\n",
    "            scores = tf.nn.xw_plus_b(h_drop, W1, b, name='scores')\n",
    "            predictions = tf.argmax(scores, 1, name='predictions')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = label,logits = scores)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            correct_predictions = tf.equal(predictions, tf.argmax(label, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "\n",
    "        # variables\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.predictions = predictions\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    TextCNN(59, 2, 100, 128, [3,4,5], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load prebuilt train data & vocab file\n",
      "load prebuilt test data & vocab file \n",
      "initialize cnn filter\n",
      "sequence length 60,  number of class 2, vocab size 49897\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least two variables have the same name: conv-maxpool-5_8/b",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-048beab2a3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-048beab2a3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step %d, loss %f, acc %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m           \u001b[0mkeep_checkpoint_every_n_hours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keep_checkpoint_every_n_hours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m           restore_sequentially=self._restore_sequentially)\n\u001b[0m\u001b[1;32m   1087\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0munique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \"\"\"\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[0;34m(self, names_to_saveables)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \"\"\"\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpListToDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mOpListToDict\u001b[0;34m(op_list)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m           raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[0;32m--> 535\u001b[0;31m                            name)\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least two variables have the same name: conv-maxpool-5_8/b"
     ]
    }
   ],
   "source": [
    "#### train & test\n",
    "from data import *\n",
    "#from textcnn import TextCNN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "TRAIN_FILENAME = 'data/ratings_train.txt'\n",
    "TRAIN_DATA_FILENAME = TRAIN_FILENAME + '.data'\n",
    "TRAIN_VOCAB_FILENAME = TRAIN_FILENAME + '.vocab'\n",
    "\n",
    "TEST_FILENAME = 'data/ratings_test.txt'\n",
    "TEST_DATA_FILENAME = TRAIN_FILENAME + '.data'\n",
    "TEST_VOCAB_FILENAME = TRAIN_FILENAME + '.vocab'\n",
    "\n",
    "def train():\n",
    "\n",
    "    if (os.path.exists(TRAIN_DATA_FILENAME) and os.path.exists(TRAIN_VOCAB_FILENAME)):\n",
    "        print('load prebuilt train data & vocab file') \n",
    "        input = load_data(TRAIN_DATA_FILENAME)\n",
    "        vocab =  load_vocab(TRAIN_VOCAB_FILENAME)\n",
    "    else:\n",
    "        print('build train data & vocab from raw text')\n",
    "        data = read_raw_data(TRAIN_FILENAME)\n",
    "        tokens = [t for d in data for t in d[0]]\n",
    "        \n",
    "        vocab = build_vocab(tokens)\n",
    "        input = build_input(data, vocab)\n",
    "\n",
    "        print('save train data & vocab file')\n",
    "        save_data(TRAIN_DATA_FILENAME, input)\n",
    "        save_vocab(TRAIN_VOCAB_FILENAME, vocab)\n",
    "    \n",
    "    if (os.path.exists(TEST_DATA_FILENAME) and os.path.exists(TEST_VOCAB_FILENAME)):\n",
    "        print('load prebuilt test data & vocab file ')\n",
    "        test_input = load_data(TEST_DATA_FILENAME)\n",
    "        test_vocab = load_vocab(TEST_VOCAB_FILENAME)\n",
    "    else:\n",
    "        print('build test data & vocab from raw text')\n",
    "        data = read_raw_data(TEST_FILENAME)\n",
    "        tokens = [t for d in data for t in d[0]]\n",
    "        \n",
    "        test_vocab = build_vocab(tokens)\n",
    "        test_input = build_input(data, test_vocab)\n",
    "\n",
    "        print('save test data & vocab file')\n",
    "        save_data(TEST_DATA_FILENAME, test_input)\n",
    "        save_vocab(TEST_VOCAB_FILENAME, test_vocab)\n",
    "\n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        seq_length = np.shape(input[0][0])[0]\n",
    "        num_class = np.shape(input[0][1])[0]\n",
    "\n",
    "        print('initialize cnn filter')\n",
    "        print('sequence length %d,  number of class %d, vocab size %d' % (seq_length, num_class, len(vocab)))\n",
    "        \n",
    "        cnn = TextCNN(seq_length, num_class, len(vocab), 128, [3,4,5], 128)\n",
    "\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        #optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input : x_batch,\n",
    "                cnn.label : y_batch,\n",
    "                cnn.dropout_keep_prob : 0.5\n",
    "            }\n",
    "\n",
    "            _, step, loss, accuracy = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "        def evaluate(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "                cnn.input : x_batch,\n",
    "                cnn.label : y_batch,\n",
    "                cnn.dropout_keep_prob : 1.0\n",
    "            }\n",
    "\n",
    "            step, loss, accuracy = sess.run([global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "            print(\"step %d, loss %f, acc %f\" % (step, loss, accuracy))\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(100):\n",
    "            try:\n",
    "                batch = random.sample(input, 64) \n",
    "            \n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 100 == 0:\n",
    "                    batch = random.sample(test_input, 64)\n",
    "                    x_test, y_test = zip(*batch)\n",
    "                    evaluate(x_test, y_test)\n",
    "                if current_step % 1000 == 0:\n",
    "                    save_path = saver.save(sess, './textcnn.ckpt')\n",
    "                    print('model saved : %s' % save_path)\n",
    "            except:\n",
    "                print (\"Unexpected error:\", sys.exc_info()[0])\n",
    "                raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "#from textcnn import TextCNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_FILENAME = 'data/ratings_train.txt'\n",
    "TRAIN_DATA_FILENAME = TRAIN_FILENAME + '.data'\n",
    "TRAIN_VOCAB_FILENAME = TRAIN_FILENAME + '.vocab'\n",
    "\n",
    "SEQUENCE_LENGTH = 60\n",
    "NUM_CLASS = 2\n",
    "\n",
    "def test():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vocab = load_vocab(TRAIN_VOCAB_FILENAME)\n",
    "        cnn = TextCNN(SEQUENCE_LENGTH, NUM_CLASS, len(vocab), 128, [3,4,5], 128)\n",
    "        #saver = tf.train.Saver()\n",
    "        \n",
    "        new_saver = tf.train.import_meta_graph('textcnn.ckpt.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "        #saver.restore(sess, eval_dir)\n",
    "        #saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "\n",
    "\n",
    "        print('model restored')\n",
    "\n",
    "        input_text = input('사용자 평가를 문장으로 입력하세요: ')\n",
    "        tokens = tokenize(input_text)\n",
    "        print('입력 문장을 다음의 토큰으로 분해:')\n",
    "        print(tokens)\n",
    "\n",
    "        sequence = [get_token_id(t, vocab) for t in tokens]\n",
    "        x = []\n",
    "        while len(sequence) > 0:\n",
    "            seq_seg = sequence[:SEQUENCE_LENGTH]\n",
    "            sequence = sequence[SEQUENCE_LENGTH:]\n",
    "\n",
    "            padding = [1] *(SEQUENCE_LENGTH - len(seq_seg))\n",
    "            seq_seg = seq_seg + padding\n",
    "\n",
    "            x.append(seq_seg)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        feed_dict = {\n",
    "            cnn.input : x,\n",
    "            cnn.dropout_keep_prob : 1.0\n",
    "        }\n",
    "\n",
    "        predict = sess.run([cnn.predictions], feed_dict)\n",
    "        result = np.mean(predict)\n",
    "        if (result > 0.75):\n",
    "            print('추천')\n",
    "        elif (result < 0.25):\n",
    "            print('비추천')\n",
    "        else:\n",
    "            print('평가 불가능')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./textcnn.ckpt\n",
      "model restored\n",
      "사용자 평가를 문장으로 입력하세요: 안녕\n",
      "입력 문장을 다음의 토큰으로 분해:\n",
      "['안녕/Noun']\n",
      "추천\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
